{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iris Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Iris dataset is a classic dataset in the field of machine learning and statistics, often used as a benchmark for classification algorithms. It consists of 150 iris flower samples from three different species: setosa, versicolor, and virginica.\n",
    "\n",
    "The task is to classify iris flowers into their correct species based on four measured features:\n",
    "Sepal length: The length of the flower's sepal in centimeters.\n",
    "Sepal width: The width of the flower's sepal in centimeters.\n",
    "Petal length: The length of the flower's petal in centimeters.\n",
    "Petal width: The width of the flower's petal in centimeters.\n",
    "\n",
    "Build a model that can accurately predict the species of an iris flower given its four measured features. A good model would have a high accuracy rate, meaning it correctly classifies a large percentage of the iris flowers in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prapt\\AppData\\Local\\Temp\\ipykernel_17960\\2870874771.py:3: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  pd.options.mode.use_inf_as_na = True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.use_inf_as_na = True\n",
    "\n",
    "import tensorflow as tf\n",
    "from logzero import logger\n",
    "# import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "data_config = {\n",
    "    \"data_path\": r\"data\\iris\\iris.data\",\n",
    "    \"preprocess\": False,\n",
    "    \"validate\": True,\n",
    "    \"validation_split\":0.2,\n",
    "    \"test_split\":0.3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPipeline:\n",
    "    def __init__(self):\n",
    "        logger.info('Data Pipeline set up')\n",
    "        self.class_mapping = {\"Iris-setosa\": 0, \"Iris-versicolor\": 1, \"Iris-virginica\": 2}\n",
    "\n",
    "    def load(self, data_path:str):\n",
    "        \"\"\"Loading Iris dataset \"\"\"\n",
    "\n",
    "        # dataset = tfds.load('iris', split='train', shuffle_files=True)\n",
    "        dataset_df = pd.read_csv(data_path, names=[ \"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"species\"]).sample(frac=1)\n",
    "        dataset_df[\"species\"] = dataset_df[\"species\"].map(self.class_mapping)\n",
    "        logger.debug( f\"Dataset length: {len(dataset_df)}\" )\n",
    "        logger.info( f\"Loaded {len(dataset_df)} records!\" )\n",
    "\n",
    "        return dataset_df\n",
    "\n",
    "    def preprocess(self, dataset):\n",
    "        \"\"\"Pre-process dataset\n",
    "\n",
    "        Args:\n",
    "            dataset (_type_): _description_\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def validate(self, dataset):\n",
    "        \"\"\"Validates dataset\n",
    "\n",
    "        Args:\n",
    "            dataset (tf.data.Dataset/np.ndarray): .\n",
    "\n",
    "        Returns:\n",
    "            isvalid (bool): Is loaded dataset valid\n",
    "            error (str): Error message in case dataset is not valid\n",
    "        \"\"\"\n",
    "        if len(dataset.columns) != 5:\n",
    "            return False, \"Dataset should have the following columns - [ sepal_length, sepal_width, petal_length, petal_width, species]. One or more columns missing.\"\n",
    "        \n",
    "        null_mask = dataset.isnull()\n",
    "        if null_mask.values.any():\n",
    "            return False, f\"Null values found at indices - {zip(np.where(null_mask))}\"\n",
    "        \n",
    "        return True, \"\"\n",
    "\n",
    "    def run(self, config:dict):\n",
    "        \"\"\"Runner for the complete pipeline\n",
    "\n",
    "        Args:\n",
    "            config (dict): .\n",
    "\n",
    "        Raises:\n",
    "            Exception: InValidDatasetException when the data schema is not valid \n",
    "        \"\"\"\n",
    "        dataset = self.load(config[\"data_path\"])\n",
    "        \n",
    "        if config.get(\"validate\", False):\n",
    "            isValid, error = self.validate(dataset)\n",
    "            if not isValid:\n",
    "                raise Exception(error)\n",
    "            else:\n",
    "                logger.info(\"Loaded dataset has a valid schema\")\n",
    "        \n",
    "        split_index = int((1-config[\"test_split\"])*len(dataset))\n",
    "        train_dataset = dataset.iloc[:split_index]\n",
    "        test_dataset = dataset.iloc[split_index:]\n",
    "        logger.debug( f\"Records in: Train split - {len(train_dataset)}, Test split - {len(test_dataset)}\" )\n",
    "\n",
    "        if config.get(\"preprocess\", False):\n",
    "            self.preprocess(train_dataset)\n",
    "        \n",
    "        return train_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 241017 22:26:45 2945258857:3] Data Pipeline set up\n",
      "[D 241017 22:26:45 2945258857:12] Dataset length: 150\n",
      "[I 241017 22:26:45 2945258857:13] Loaded 150 records!\n",
      "[I 241017 22:26:45 2945258857:60] Loaded dataset has a valid schema\n",
      "[D 241017 22:26:45 2945258857:65] Records in: Train split - 105, Test split - 45\n"
     ]
    }
   ],
   "source": [
    "data_pipeline = DataPipeline()\n",
    "train_dataset, test_dataset = data_pipeline.run(data_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingPipeline:\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferPipeline:\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_WS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
